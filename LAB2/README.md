# Лабораторная работа №2. Реализация глубокой нейронной сети

### `Герчик Артём Вадимович, ИиТП(ОБОИ) 1-й курс, группа 456241.`

## Необходимые иструменты

- `python 3.9.0`
- `виртуальная среда`
- `внешние библиотеки`
- [большой набор данных](https://commondatastorage.googleapis.com/books1000/notMNIST_large.tar.gz)
- [малый набор данных](https://commondatastorage.googleapis.com/books1000/notMNIST_small.tar.gz)

```bash
# Установка виртуальной среды
python -m venv venv

# Активация виртуальной среды

# Windows
venv\Scripts\activate
# macOS and Linux
source venv/bin/activate

# Установка внешних зависимостей
pip install -r requirements.txt
```

## Использование

#### `После запуска приложения необходимо указать путь до набора данных, после чего, программа проведет отбор данных, чтобы убрать поврежденные картинки, и уже потом последовательно выполняются задания.`

## Результаты

### `Task_1 Model Summary:`

`Model: sequential`

```
Layer (type)                Output Shape              Param

flatten (Flatten)           (None, 784)               0

dense (Dense)               (None, 256)               200960    

dense_1 (Dense)             (None, 128)               32896     

dense_2 (Dense)             (None, 64)                8256      

dense_3 (Dense)             (None, 10)                650
```

### `Task_1 accuracy: 0.9009`

#### `В задании 1 была создана модель с пятью слоями и десятью эпохами обучения. Удалось достигнуть точности в ~90%.`

#### `В ответ на вопрос, поставленный в задаче 2, можно сказать следующее: Точность значительно увеличилась. Связано это с использованием нескольких слоев для построения нейронной сети, в отличии от единственной классифицирующей модели, которая была реализована и использована в лабораторной работе 1.`

### `Task_3 Model Summary:`

`Model: sequential`

```
Layer (type)                Output Shape              Param

flatten (Flatten)           (None, 784)               0

dense (Dense)               (None, 256)               200960    

dense_1 (Dense)             (None, 128)               32896     

dense_2 (Dense)             (None, 64)                8256      

dropout (Dropout)           (None, 64)                0

dense_3 (Dense)             (None, 10)                650
```

### `Task_3 accuracy: 0.8952`

#### `В задании 3 был добавлен Dropout метод, который предназначен для того, чтобы с некой вероятностью отключать нейроны для предотвращения переобучения модели. В результате получаем точность, не превышающую точность модели из первого задания.`
#### `Можно сделать вывод, что переобучения с настоящим набором данных на десяти эпохах не происходит, скорее даже метод Dropout является лишним в данном случае. Метод Dropout может временно понизить точность модели, даже если переобучения не произошло. Это связано с тем, что случайное отключение нейронов во время обучения может привести к тому, что модель не будет использовать все доступные ей ресурсы для обучения на каждом шаге. Однако это временное снижение точности обычно компенсируется улучшением способности модели к обобщению на новых данных.`

### `Task_4 Model Summary:`

`Model: sequential`

```
Layer (type)                Output Shape              Param

flatten (Flatten)           (None, 784)               0

dense (Dense)               (None, 256)               200960    

dense_1 (Dense)             (None, 128)               32896     

dense_2 (Dense)             (None, 64)                8256      

dropout (Dropout)           (None, 64)                0

dense_3 (Dense)             (None, 10)                650 
```

### `Task_4 accuracy: 0.9036`

#### `В задании 4 была внедрена функция динамической скорости обучения. Данная функция снижает скорость обучения по мере прогресса обучения модели. Это помогает модели стабилизироваться и улучшить точность, избегая слишком больших шагов, которые могут привести к колебаниям вокруг минимума функции потерь. Было принято решение снижать скорость обучения при наступлении шестой эпохи.`

#### `В результате, точность немного повысилась, однако уровня в 97.1% не достигла. Вероятно, для достижения данного уровня точности, необходимо увеличить количество эпох обучения.`